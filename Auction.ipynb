{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGWoPDWce4t0y+QGvNP6g7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JiuqinWei/BidOptimizerDL/blob/main/Auction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Q-Learning\n",
        "\n",
        "Below is a deep Q network implementation of the Asynchronous updating rule in section IV of this [paper](https://elsevier-ssrn-document-store-prod.s3.amazonaws.com/23/12/11/ssrn_id4660391_code374067.pdf?response-content-disposition=inline&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEGUaCXVzLWVhc3QtMSJIMEYCIQDLr7V64Gk%2FQ51%2FCGVUmtxMgzYc8p7EJgcboIwtPROqeQIhAMNlbgXAq9FbC5J20SYTW3R2On3D5yoQqQ7d%2FvLNaJhuKr0FCE4QBBoMMzA4NDc1MzAxMjU3Igzgq0tELmuXZVaUj40qmgXNJOfcy3qLeCmYa8e%2FaZVJwNt4PNpdpZ2jzMU%2BMmmLA28fzQCMwWm%2FFDrZcqMa0JVJeSfUFfnBzXOBBnrTab9J5dsF8qFq7zsBLIIIAoUj%2B84UOUF08iheFSR7qdXqKO8TeXgDvrsMcyCv8DcAEHmT9zVkg%2FjvHABLiOP8Roxr5d1KJaso4RSm0EHXHHQ2PCDiP8ehf%2BwjaCHz50SmQsK%2BhuSSdXrbkTADQFH7%2F%2Favadih6fH1agkUxCfsNwTJ%2Fg76n527eYNW55pn4o%2BqZlO%2BkN6qClVV8I0FIFruUpkyshbRBPiN7BCaO4xs%2Bw%2BfFPxILsTGyAv9wYT3J%2FokFgt1HMyOF0gYhpvMZIzUtyhBnR438LzR5UDsS2uqN9pc%2B%2FDdNo68lLIo%2FdSzCiV8MEdX3G4gUPLUqRKFygwkJQniYPvs8%2BcniapxPd6ZGrGU%2F2BPvl6d%2FRLotnzH%2BA3qTsefvm%2FvSsNmwxnELdhK1f2Qo7ZQFI%2FfDh6qQmQPYJar2t2ZJAz3CJxMTcz2mFRKT8iSSeA58FUtBVgeDUPhqC9saPnuQNjcw03wooN1FrZEATpNGqNPuhzMyAXVlYSmJYg5fDaX9EI8N5NAA43znuQ8FeEtcJE7O%2Fx4pxnDa4MwsG9SztxGpqkhqZfcEdU6Y5TNw1qq0P3NP23QzD26TOLOMJE1U%2B3sQKwafKL3%2FIK4vBBCT7jNHOGnIFNO9sPDoD%2BPfvmvqBzCjf3NMat7isqOv9cXgmauj1B7j9rynXS52tDwKPHKK8uVXdjTU16vw3KoJsJxLQ5Kw2OGEADRQlQrvvdTVl%2F6iMWbTEdKeJlCvyDRjEkBjcVpPFlPTKQyCy3mViThNy7QtwpWF3J0R6a0FD0ASIDlJh1mVL4w8qmltQY6sAG1H5LcqxyljWQ5uDwnjf6hpkt3OqZXxfT0NMBkNQZf975AsKhOsFXN7kvdVoE%2BcheVrLPomrfGzauY8AOGK3AWymz3F14%2FuuEGOvSQ47Fi2wX2AK2rurhLHCNoc%2BkCo1q3lGA%2Bt%2BJYLcKbCSaEvvLFjGCud6BgIk6ECnMo21CdG9t92EzUdVsaE2JfPJc6l1ZSmIm9ZDaEN8W1%2BZG7NYSXD0%2BPCGjw4Po78SWI6yamBg%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240730T222155Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAUPUUPRWETVLAEV4L%2F20240730%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=d7fa2820375dd3317f0230e5f53a2a59ae507ac74ddef3b9106c9a90de36c77b)\n"
      ],
      "metadata": {
        "id": "adDX-gLCLY5x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall the Asynchronous Rule for the $i$-th player:\n",
        "\n",
        "  $$Q(s_t, b^i_t) = (1-\\alpha)\\cdot Q(s_t, b^i_t) + \\alpha \\cdot (r(s_t,b^i_t, b^{-i}_t) + \\delta * max_{b^i\\in B^i} Q(s_{t+1}, b^i))$$\n",
        "\n",
        "Here, the $Q$ function intends to ressemble the optimal function $Q^*: S\\times A\\to \\mathbb{R}$ that maximizes the (discounted) cumulative rewards following some specific policy. Here $S$ is the set of states and $A$ the set of actions.\n",
        "\n",
        "We will construct two players playing the auction game. Each player is represented by a neural network. For each player, we also create an additional network representing their \"future\": $max_{b^i\\in B^i} Q(s_{t+1}, b^i))$ (the idea is based on this [post](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html#hyperparameters-and-utilities)).\n",
        "\n",
        "The reasoning behind this is that in the above equation, $Q(s_{t+1}, b^i)$ is not the same as $Q(s_t, b^i_t)$ because the second player may vary the bidding choice based on the value of $b^i_t$ in this round.\n",
        "\n",
        "(Instead, it might be possible to simply predict the \"future\" by running the model for the second player to get a bid, and then run the model for the first player on $(s_{t+1}, b^i)$. But I am not sure if this will violate any logic)"
      ],
      "metadata": {
        "id": "JnK51L6nV-lx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfp0SJHDUPSa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_player = 2\n",
        "n_bids = 10  # number of bids a player can choose from\n",
        "n_input = n_player\n",
        "n_hidden = 32\n",
        "n_output = 1\n",
        "\n",
        "TAU = 0.005 # soft updating rule for the future model\n",
        "delta = 0.95 # discount factor in the updating rule\n",
        "beta  = 1.22e-05 # variable for epsilon greedy algorithm\n",
        "\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "bjWyQhP6if3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Below is some ad-hoc function I wrote. They can be replaced by the functions written by whoever takes in charge of this part.)"
      ],
      "metadata": {
        "id": "bgrROPQj2sy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #For simplicity, we assume the biddings of all players are distinct.\n",
        "\n",
        "# def get_price(bids, n):\n",
        "#   #Input:\n",
        "#   #   biddings: the bid for each player\n",
        "#   #   n: the number of slots to bid for\n",
        "#   #Output:\n",
        "#   #   ranked_players: a list of index of player ranked from the highest bid to the lowest\n",
        "#   #   prices: the prices each player in ranked_player has to pay\n",
        "#   if (n > len(bids)):\n",
        "#     print(\"n must be smaller than the length of bids\")\n",
        "#     return\n",
        "#   prices = np.zeros(len(bids))\n",
        "#   ranked_players = np.zeros(len(bids))\n",
        "#   #This returns a list of tuple. The first entry is the index of the player and the second entry is the corresponding bids\n",
        "#   ranked_bids = sorted(enumerate(bids), key = lambda x: x[1], reverse = True)\n",
        "#   for i in range(n):\n",
        "#     ranked_players[i] = ranked_bids[i][0]\n",
        "#     prices[i] = ranked_bids[i+1][1] if i+1 < n else 0\n",
        "#   #return upto the number of slots\n",
        "# return ranked_players[:n], prices[:n]\n",
        "\n",
        "# def get_profits(values, clicks, prices, ranked_players):\n",
        "#   #Input:\n",
        "#   #   values: a list of the value of the slot per click for each player\n",
        "#   #   clicks: a list of the number of click each slot will get\n",
        "#   #   ranked_players: a list of index of player ranked from the highest bid to the lowest\n",
        "#   #   prices: the prices each player in ranked_player has to pay\n",
        "#   #Output:\n",
        "#   #   profits: a list of profits each player obtains\n",
        "#   profits = np.zeros(len(values))\n",
        "#   for rank, player in enumerate(ranked_players):\n",
        "#     rank = int(rank)\n",
        "#     player = int(player)\n",
        "#     profits[player] = (values[rank] - prices[rank])*clicks[rank]\n",
        "#   return profits\n",
        "\n",
        "# #Simplified version for a single player\n",
        "# def get_reward(value, click, price, rank):\n",
        "#   reward = (values[rank] - prices[rank])*clicks[rank]\n",
        "#   return reward\n",
        "\n",
        "# def bid_sampler(bids_distribution):\n",
        "#   #Input:\n",
        "#   #   bids_distribution: a list storing the possible bids of a player\n",
        "#   #Output:\n",
        "#   #   bid: the bid of the player\n",
        "#   bid = np.random.choice(bids_distribution)\n",
        "#   return bid"
      ],
      "metadata": {
        "id": "b5fYH1prz-eE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ranked_players, prices = get_price([1,2],2)\n",
        "# values = [4,2]\n",
        "# clicks = [2,1]\n",
        "# profits = get_profit(values, clicks, prices, ranked_players)\n",
        "# for i in range(len(profits)):\n",
        "#   print(\"Player {} obtains profit {}\".format(i, profits[i]))"
      ],
      "metadata": {
        "id": "Y4_VlbHx0hQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deep Q network that models the Q function.\n",
        "\n",
        "In the paper, the algorithm uses the Q matrix to represent the expected reward for a state and action pair. Instea, we use a neural network to represent this Q matrix.\n",
        "\n",
        "Input: the biddings of other players in the last round as state, and the current bid as action.\n",
        "\n",
        "Output: the expected reward (reward in the long\n",
        "run)."
      ],
      "metadata": {
        "id": "ySGlgFjDOg3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, n_observations, n_output):\n",
        "        super(AgentModel, self).__init__()\n",
        "        self.layer1 = nn.Linear(n_observations, n_hidden)\n",
        "        self.layer2 = nn.Linear(n_hidden, n_hidden)\n",
        "        self.layer3 = nn.Linear(n_hidden, n_output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        return self.layer3(x)"
      ],
      "metadata": {
        "id": "ovptCZ-TUd0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create two agents as deep Q networks"
      ],
      "metadata": {
        "id": "CR2CmHSzW0QM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Agent0 = DQN(n_input, n_output)\n",
        "Future0 = DQN(n_input, n_output)\n",
        "Future0.load_state_dict(Agent0.state_dict())\n",
        "\n",
        "Agent1 = DQN(n_input, n_output)\n",
        "Future1 = DQN(n_input, n_output)\n",
        "Future1.load_state_dict(Agent1.state_dict())\n",
        "\n",
        "Agents = [Agent0, Agent1]\n",
        "Futures = [Future0, Future1]\n",
        "\n",
        "Bids = [] # List of arrays of possible bids of each player"
      ],
      "metadata": {
        "id": "WT663jna8Z5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an optimizer for each player\n",
        "optimizer0 = optim.AdamW(Agent0.parameters(), lr=learning_rate, amsgrad=True)\n",
        "optimizer1 = optim.AdamW(Agent1.parameters(), lr=learning_rate, amsgrad=True)\n",
        "optimizers = [optimizer0, optimizer1]"
      ],
      "metadata": {
        "id": "mAmfwE0wvQ7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bid is selected using epsilon-greedy algorithm.\n",
        "\n",
        "When the learning starts (i.e. step is small), the algorithm explores more options rather than choosing the optimal bid. As time progresses, the algorithm chooses the optimal bid more and more often.\n",
        "\n",
        "(The optimal bid is selected by running the model (with the knowledge of bidding from last round) for all possible bids and than select the bid that gives the best value)"
      ],
      "metadata": {
        "id": "n24i0z76VUsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#epsilon greedy algorithm to choose the bidding based on last round observation\n",
        "def select_bid(index_player, state, bids, step):\n",
        "  #Input:\n",
        "  #   index_player: an integer indicating the index of player\n",
        "  #   states: an array of length n-1 of the bids by the first n-1 players in the last round\n",
        "  #   bids: possible bidding of the player\n",
        "  #   step: variable controlling the decay rate of epsilon greedy algorithm\n",
        "\n",
        "  Agent = Agents[index_player]\n",
        "  Future = Futures[index_player]\n",
        "  epsilon = np.random.choice([0,1], p=[np.exp(-beta*step), 1-np.exp(-beta*step)])\n",
        "\n",
        "  #compute the expected reward given the bids of other players\n",
        "  expected_rewards = []\n",
        "  for i in range(len(bids)):\n",
        "    state = states.append(bids[i])\n",
        "    expected_rewards.append(Agent(state))\n",
        "  optimal_bid = bids[np.argmax(expected_rewards)]\n",
        "\n",
        "  bid = optimal_bid if epsilon == 1 else bids[np.random.randint(0, len(bids) - 1)]\n",
        "  return bid"
      ],
      "metadata": {
        "id": "_Wec7s1HEE0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since every $Q$ function for some policy obeys the Bellman equation. We can write the loss function as the \"temporal difference\". If $L = 0$ , then we know that our model is modeling the optimal $Q^*$ function; hence the smaller the $L$, the closer our network models $Q^*$.\n",
        "\n",
        "$$L = Q(s_t, b^i_t) - (r(s_t,b^i_t, b^{-i}_t) + \\delta * max_{b^i\\in B^i} Q(s_{t+1}, b^i))$$"
      ],
      "metadata": {
        "id": "IJHYlyUGKJ9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get bid, reward, and loss based on state (previous bidding). It could be used to plot training graph\n",
        "def get_bid_reward_loss(index_player, state):\n",
        "    Agent = Agents[index_player]\n",
        "    Future = Futures[index_player]\n",
        "    bids = Bids[index_player]\n",
        "\n",
        "    # Compute all possible combinations\n",
        "    expected_reward = Agent(state)\n",
        "    bid = select_bid(index_player, state, bids, step)\n",
        "    reward = get_reward(state, bid)\n",
        "    loss = expected_reward - (reward + delta*max([Future(state.append(bid)) for bid in range(len(bids))]))\n",
        "    return bid, reward, loss"
      ],
      "metadata": {
        "id": "Coltgism3n0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to make one optimization step.\n",
        "def optimize_model(index_player):\n",
        "  bids = Bids[index_player]\n",
        "  optimizer = optimizers[index_player]\n",
        "\n",
        "  max_reward = 0\n",
        "  bid,reward,loss = get_bid_reward_loss(index_player, state)\n",
        "\n",
        "  # Optimize the model\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  # In-place gradient clipping\n",
        "  #torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "id": "ydon9G3EmB65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The training phase\n",
        "steps = 100000\n",
        "\n",
        "states = [] #list of arrays to store the previous states\n",
        "rewards = []\n",
        "losses = []\n",
        "\n",
        "for step in range(steps):\n",
        "  #Get current state: the biddings of the last round\n",
        "  state = states[step]\n",
        "\n",
        "  #Get the bids of each player for this round\n",
        "  bid0, reward0, loss0 = get_bid_reward_loss(0, state)\n",
        "  bid1, reward1, loss1 = get_bid_reward_loss(1, state)\n",
        "\n",
        "  # Perform one step of the optimization\n",
        "  optimize_model(0)\n",
        "  optimize_model(1)\n",
        "\n",
        "  # Soft update of the future network's weights\n",
        "  # θ′ ← τ θ + (1 −τ )θ′\n",
        "  for i in range(2):\n",
        "    Agent = Agents[i]\n",
        "    Future = Futures[i]\n",
        "    Future_state_dict = Future.state_dict()\n",
        "    Agent_state_dict = Agent.state_dict()\n",
        "    for key in Agent_state_dict:\n",
        "        Future_state_dict[key] = Agent_state_dict[key]*TAU + Future_state_dict[key]*(1-TAU)\n",
        "    Future.load_state_dict(Future_state_dict)\n",
        "\n",
        "  #Update state, reward, loss\n",
        "  states.append([bid0, bid1])\n",
        "  rewards.append([reward0, reward1])\n",
        "  losses.append([loss0, loss1])"
      ],
      "metadata": {
        "id": "MhmVi5AKmFtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Model\n",
        "\n",
        "This is intended to be the naive model based on our meeting.\n",
        "The model takes in all possible bids of the other players and output an optimal bid for the current round."
      ],
      "metadata": {
        "id": "LZ-LGwvbVeBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_players = 3\n",
        "n_bids = 100\n",
        "n_input = (n_players - 1)*(n_bids)\n",
        "n_hidden = 32\n",
        "n_output = 1"
      ],
      "metadata": {
        "id": "xkNBZbCWcmHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bid_1 = [i/100 for i in range(100)]\n",
        "# bid_2 = [i/100 for i in range(100)]"
      ],
      "metadata": {
        "id": "OqbnKmRacGwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To be modified.\n",
        "def get_state():\n",
        "  return state\n",
        "def get_reward():\n",
        "  return reward"
      ],
      "metadata": {
        "id": "y3fI0V8pYHdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentModel(nn.Module):\n",
        "    def __init__(self, n_input, n_output):\n",
        "        super(AgentModel, self).__init__()\n",
        "        self.layer1 = nn.Linear(n_input, n_hidden)\n",
        "        self.layer2 = nn.Linear(n_hidden, n_output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        return self.layer2(x)"
      ],
      "metadata": {
        "id": "W7AdH4_bVe6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Agent0 = AgentModel(n_input, n_output)"
      ],
      "metadata": {
        "id": "th_wrpzEWFBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.AdamW(Agent0.parameters(), lr=learning_rate, amsgrad=True)"
      ],
      "metadata": {
        "id": "fk0TqhNrVvJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_model():\n",
        "  # Compute all possible combinations\n",
        "  max_reward = 0\n",
        "  for i in range(len(bid_1)):\n",
        "    new_reward = get_reward()\n",
        "    max_reward = new_reward if new_reward > max_reward else max_reward\n",
        "  # Compute loss\n",
        "  loss = max_reward - reward\n",
        "\n",
        "  # Optimize the model\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  # In-place gradient clipping\n",
        "  #torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "id": "wC79o8TQWAPX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}